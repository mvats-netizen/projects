# AI-Led Curations - Content Discovery Pipeline

A semantic search pipeline for discovering and curating learning content at the item level (videos, readings, labs) using token-based embeddings with contextual metadata pre-pending.

## ğŸ¯ Problem Statement

Coursera's catalog has 16,000+ courses, but users struggle to find specific content due to:
- Limited search that only works at course level
- No visibility into item-level content (specific videos, readings)
- Existing skill metadata is incomplete

## ğŸ’¡ Solution

This pipeline enables granular content discovery by:
1. **Fetching real transcripts & readings** from Databricks with domain/language filtering
2. **Chunking content** with 750-token sliding windows and 150-token overlap
3. **Contextual pre-pending** with course/module/level metadata before embedding
4. **LLM-extracted metadata** (Bloom's level, skills, cognitive load, prerequisites)
5. **Semantic search** with item-level deduplication
6. **Interactive UI** with skill confirmation and YouTube-style content previews

### Example Queries

| Query | What Gets Retrieved |
|-------|---------------------|
| "What is a pivot table?" | Exact video explaining pivot tables |
| "pivot table from multiple sheets" | Video segment specifically about multi-sheet pivot tables |
| "Machine learning for biomedical" | Curated set of ML videos with healthcare context |

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DATA INGESTION                                                                      â”‚
â”‚                                                                                      â”‚
â”‚  Databricks â”€â”€â”¬â”€â”€ Video Subtitles â”€â”€â†’ Transcripts                                   â”‚
â”‚               â””â”€â”€ Reading Materials â”€â”€â†’ Content                                      â”‚
â”‚                                                                                      â”‚
â”‚  Filters: Domain (Software Engineering) | Language (English) | Status (Live)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  METADATA ENRICHMENT                                                                  â”‚
â”‚                                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Operational Metadata  â”‚    â”‚  Derived Metadata (LLM Extraction)             â”‚    â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚    â”‚
â”‚  â”‚  â€¢ Course Duration     â”‚    â”‚  â€¢ Atomic Skills (3-5 per chunk)               â”‚    â”‚
â”‚  â”‚  â€¢ Module Count        â”‚    â”‚  â€¢ Primary/Sub Domain                          â”‚    â”‚
â”‚  â”‚  â€¢ Instructor Name     â”‚    â”‚  â€¢ Bloom's Cognitive Level                     â”‚    â”‚
â”‚  â”‚  â€¢ Partner Name        â”‚    â”‚  â€¢ Cognitive Load (Low/Medium/High)            â”‚    â”‚
â”‚  â”‚  â€¢ Difficulty Level    â”‚    â”‚  â€¢ Instructional Function                      â”‚    â”‚
â”‚  â”‚  â€¢ Last Updated        â”‚    â”‚  â€¢ Prerequisite Concepts                       â”‚    â”‚
â”‚  â”‚  â€¢ Pass Rate           â”‚    â”‚  â€¢ Key Entities/Concepts                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INDEXING PIPELINE                                                                    â”‚
â”‚                                                                                       â”‚
â”‚  Content â”€â”€â†’ Token Chunking â”€â”€â†’ Context Pre-pending â”€â”€â†’ Embed â”€â”€â†’ FAISS HNSW Index   â”‚
â”‚              (750 tokens,        [Course: X]                                          â”‚
â”‚               150 overlap)       [Module: Y]                                          â”‚
â”‚                                  [Level: Z]                                           â”‚
â”‚                                  {Transcript}                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SEARCH & RETRIEVAL                                                                   â”‚
â”‚                                                                                       â”‚
â”‚  User Query â”€â”€â†’ Skill Extraction â”€â”€â†’ Embed â”€â”€â†’ Vector Search â”€â”€â†’ Deduplicate by Item â”‚
â”‚                      â†“                                                â†“               â”‚
â”‚               Taxonomy Match                              Ranked Results with         â”‚
â”‚               (2000+ skills)                              â€¢ Item Name & Link          â”‚
â”‚                      â†“                                    â€¢ Lesson & Module           â”‚
â”‚               Skill Confirmation                          â€¢ Content Preview           â”‚
â”‚                                                           â€¢ Confidence Score          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Installation

```bash
cd content-curations
pip install -r requirements.txt

# Download spaCy model (optional, for advanced sentence splitting)
python -m spacy download en_core_web_sm
```

### Configuration

Create `config/secrets.env` from the example:

```bash
cp config/secrets.env.example config/secrets.env
```

Configure your credentials:

```bash
# Databricks (for data loading)
DATABRICKS_HOST=your-databricks-host
DATABRICKS_TOKEN=your-access-token
DATABRICKS_HTTP_PATH=/sql/1.0/warehouses/your-warehouse-id

# Embedding provider (choose one)
EMBEDDING_PROVIDER=local  # Options: local, openai, gemini
EMBEDDING_MODEL=all-MiniLM-L6-v2

# API Keys (if using cloud embeddings or LLM extraction)
GOOGLE_API_KEY=your-gemini-key
OPENAI_API_KEY=your-openai-key
```

### Running the Pipeline

#### 1. Test Databricks Connection

```bash
python scripts/test_databricks_connection.py
```

#### 2. Build the Search Index

```bash
python scripts/build_index.py
```

#### 3. Extract LLM Metadata (Optional)

```bash
python scripts/extract_metadata.py
```

#### 4. Launch the UI

```bash
streamlit run app.py --server.port 8501
```

### Basic Usage (Programmatic)

```python
from src.pipeline import TranscriptSearchPipeline
from src.skills import SkillExtractor

# Initialize pipeline with local embeddings
pipeline = TranscriptSearchPipeline(
    provider="local",
    model="all-MiniLM-L6-v2",
)

# Load pre-built index
pipeline.load_index("data/index")

# Initialize skill extractor for UI display
skill_extractor = SkillExtractor()

# Extract skills from query
query = "How do I create a pivot table from multiple sheets?"
skills = skill_extractor.extract_skills(query)
print(f"Detected skills: {skills.matched_skills}")

# Search with item-level deduplication
results = pipeline.search(query, top_k=10, deduplicate_by_item=True)

for result in results:
    print(f"Score: {result['score']:.3f}")
    print(f"Item: {result['item_name']}")
    print(f"Lesson: {result['lesson_name']}")
    print(f"Course: {result['course_name']}")
    print(f"Type: {result['content_type']}")
    print(f"Preview: {result['chunk_text'][:200]}...")
    print()
```

## ğŸ“ Project Structure

```
content-curations/
â”œâ”€â”€ app.py                          # Streamlit UI
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.yaml               # Pipeline configuration
â”‚   â”œâ”€â”€ secrets.env                 # API keys (gitignored)
â”‚   â”œâ”€â”€ secrets.env.example         # Template for secrets
â”‚   â””â”€â”€ databricks.env.example      # Databricks config template
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ chunking/
â”‚   â”‚   â”œâ”€â”€ sentence_chunker.py     # Sentence-level chunking
â”‚   â”‚   â””â”€â”€ transcript_chunker.py   # Token-based sliding window
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â””â”€â”€ embedding_pipeline.py   # OpenAI/Gemini/Local embeddings
â”‚   â”œâ”€â”€ vector_store/
â”‚   â”‚   â””â”€â”€ faiss_store.py          # FAISS HNSW storage & search
â”‚   â”œâ”€â”€ parsers/
â”‚   â”‚   â””â”€â”€ subtitle_parser.py      # SRT/VTT parsing
â”‚   â”œâ”€â”€ skills/
â”‚   â”‚   â””â”€â”€ skill_extractor.py      # KeyBERT + taxonomy matching
â”‚   â”œâ”€â”€ metadata/
â”‚   â”‚   â”œâ”€â”€ schema.py               # Pydantic metadata models
â”‚   â”‚   â”œâ”€â”€ operational_loader.py   # Load from CourseCatalogue.xlsx
â”‚   â”‚   â””â”€â”€ llm_extractor.py        # Gemini-based metadata extraction
â”‚   â”œâ”€â”€ search/
â”‚   â”‚   â””â”€â”€ search_engine.py        # Vector search with filtering
â”‚   â”œâ”€â”€ data_loaders/
â”‚   â”‚   â””â”€â”€ databricks_loader.py    # Databricks SQL connector
â”‚   â”œâ”€â”€ config.py                   # Configuration loader
â”‚   â””â”€â”€ pipeline.py                 # Main orchestration pipeline
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build_index.py              # Build FAISS index
â”‚   â”œâ”€â”€ extract_metadata.py         # LLM metadata extraction
â”‚   â””â”€â”€ test_databricks_connection.py
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ basic_usage.py              # Usage examples
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ CourseCatalogue.xlsx        # Operational metadata source
â”‚   â”œâ”€â”€ sample_courses_content.json # Fetched transcripts/readings
â”‚   â”œâ”€â”€ sample_courses_enriched.json# With LLM-extracted metadata
â”‚   â”œâ”€â”€ index/                      # FAISS index files
â”‚   â”‚   â”œâ”€â”€ faiss.index
â”‚   â”‚   â”œâ”€â”€ chunks.json
â”‚   â”‚   â””â”€â”€ embeddings.npy
â”‚   â””â”€â”€ taxonomy/
â”‚       â””â”€â”€ coursera_skills.json    # 2000+ skills taxonomy
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ METADATA_PIPELINE.md        # Pipeline documentation
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## âš™ï¸ Configuration

### Chunking Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `window_size` | 750 | Token window size for chunking |
| `overlap` | 150 | Token overlap between chunks |
| `min_chunk_tokens` | 50 | Minimum tokens for valid chunk |

### Embedding Options

| Provider | Model | Dimensions | Cost |
|----------|-------|------------|------|
| Local | `all-MiniLM-L6-v2` | 384 | Free |
| Local | `e5-large-v2` | 1024 | Free |
| OpenAI | `text-embedding-3-small` | 1536 | $0.02/1M tokens |
| Gemini | `models/gemini-embedding-001` | 3072 | Free tier available |

### Metadata Schema

#### Operational Metadata (from CourseCatalogue.xlsx / Databricks)

| Field | Source | Purpose |
|-------|--------|---------|
| Course Duration | `total_video_seconds` | Time-based filtering |
| Module Count | `count(module_id)` | Course depth indicator |
| Instructor Name | `instructor_name` | Expert filtering |
| Partner Name | `partner_name` | Brand trust (Stanford, Google) |
| Difficulty | `catalog_difficulty` | Skill-level matching |
| Last Updated | `content_last_updated` | Freshness filtering |
| Pass Rate | `assessment_pass_percentage` | Quality indicator |

#### Derived Metadata (LLM Extracted via Gemini)

| Field | Method | Purpose |
|-------|--------|---------|
| Atomic Skills | Transcript analysis | Primary matching criteria |
| Domain/Sub-Domain | Zero-shot classification | Search space narrowing |
| Bloom's Level | Cognitive verb detection | Intent matching (know vs. do) |
| Cognitive Load | Jargon frequency analysis | Learner level matching |
| Instructional Function | Teaching method categorization | Style matching |
| Prerequisites | Concept dependency detection | Knowledge gap identification |
| Key Concepts | Entity extraction | Hyper-specific retrieval |

## ğŸ–¥ï¸ Streamlit UI Features

The interactive UI (`app.py`) provides:

1. **Natural Language Search** - Enter queries like "What is a pivot table?"
2. **Skill Confirmation** - Review and confirm extracted skills before search
3. **YouTube-Style Results** - Card layout with:
   - Content preview (video/reading description)
   - Item name and type (Video/Reading)
   - Lesson and module context
   - Course name and partner
   - Confidence score with visual indicator
4. **Deduplication** - One result per item (best matching chunk)

## ğŸ”„ Data Flow

```
1. DATA LOADING
   Databricks â†’ Filter (Domain: Software Dev, Language: English)
              â†’ Fetch Videos (subtitles) + Readings (content)
              â†’ Store as sample_courses_content.json

2. METADATA ENRICHMENT (Optional)
   sample_courses_content.json â†’ LLM Extraction (Gemini)
                               â†’ Store as sample_courses_enriched.json

3. INDEX BUILDING
   Content â†’ Token Chunking (750/150)
          â†’ Context Pre-pending
          â†’ Local Embeddings (all-MiniLM-L6-v2)
          â†’ FAISS HNSW Index â†’ data/index/

4. SEARCH
   User Query â†’ Skill Extraction (KeyBERT + Taxonomy)
             â†’ Query Embedding
             â†’ Vector Similarity Search
             â†’ Deduplicate by Item
             â†’ Return Top Results with Metadata

5. UI DISPLAY
   Results â†’ Format as Cards
          â†’ Show Preview, Metadata, Confidence
          â†’ User Feedback (planned)
```

## ğŸ“Š Performance Considerations

| Dataset Size | Recommended Index | Search Latency |
|--------------|-------------------|----------------|
| < 100K chunks | `Flat` (exact) | < 10ms |
| 100K - 1M chunks | `HNSW` | < 50ms |
| > 1M chunks | `IVFFlat` + `HNSW` | < 100ms |

## ğŸ›£ï¸ Roadmap

- [x] Token-based chunking with sliding window
- [x] Contextual pre-pending for embeddings
- [x] OpenAI/Gemini/Local embedding support
- [x] FAISS vector storage
- [x] SRT/VTT subtitle parsing
- [x] Databricks data loading
- [x] LLM metadata extraction (Gemini)
- [x] Skill extraction & taxonomy matching
- [x] Streamlit UI with YouTube-style cards
- [x] Item-level deduplication
- [ ] Metadata filtering in search
- [ ] Chat interface with FSM
- [ ] Curation pathway builder
- [ ] Feedback loop integration
- [ ] A/B testing framework

## ğŸ“š References

- [AI-Led Curations PRD](./AI%20Led%20Curations_PRD.pdf)
- [AI-Led Curations TFD](./AI%20Led%20Curations%20_TFD.pdf)
- [Metadata Pipeline Documentation](./docs/METADATA_PIPELINE.md)

## ğŸ”§ Troubleshooting

### Common Issues

**Databricks Connection Failed**
```bash
# Verify credentials in config/secrets.env or parent .env
# Test connection:
python scripts/test_databricks_connection.py
```

**Embedding Dimension Mismatch**
```bash
# Delete old index and rebuild:
rm -rf data/index/*
python scripts/build_index.py
```

**Python 3.9 Type Hint Errors**
```bash
# The codebase uses typing.Union and typing.List for Python 3.9 compatibility
# No action needed if using Python 3.9+
```

## License

Internal Coursera Project
